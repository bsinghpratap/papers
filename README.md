# List of papers presented during Journal Club | BioNLP Lab, UMass-Amherst.

- Improved Pretraining for Domain-specific Contextual Embedding Models: [LINK](https://arxiv.org/abs/2004.02288)

- Gradient Episodic Memory for Continual Learning: [LINK](https://papers.nips.cc/paper/7225-gradient-episodic-memory-for-continual-learning.pdf)

- THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS: [LINK](https://www.thetalkingmachines.com/sites/default/files/2019-05/2c35994ea2912e6517a87c50fc55faa58f0df150-compressed.pdf)

- Span-based Joint Entity and Relation Extraction with Transformer Pre-training: [LINK](https://arxiv.org/pdf/1909.07755.pdf)

- CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning: [LINK](https://arxiv.org/pdf/1911.10438.pdf)

- Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction: [LINK](https://arxiv.org/pdf/1911.09886.pdf)

- Composition-based Multi-Relational Graph Convolutional Networks: [LINK](https://arxiv.org/abs/1911.03082)

- Linguistically-Informed Self-Attention for Semantic Role Labeling: [LINK](https://arxiv.org/abs/1804.08199)

- Attention Guided Graph Convolutional Networks for Relation Extraction: [LINK](https://arxiv.org/abs/1906.07510)

- When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion: [LINK](https://www.aclweb.org/anthology/P19-1116/), [BLOG](https://lena-voita.github.io/posts/acl19_context.html)

- Probing Neural Network Comprehension of Natural Language Arguments: [LINK](https://arxiv.org/pdf/1907.07355.pdf)

- SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability: [LINK](https://arxiv.org/abs/1706.05806)

- Attentive History Selection for Conversational Question Answering: [LINK](https://arxiv.org/abs/1908.09456)

- On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation: [LINK](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140#sec019)

- Are Sixteen Heads Really Better than One?: [LINK](https://arxiv.org/pdf/1905.10650.pdf)

- Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned: [LINK](https://www.aclweb.org/anthology/P19-1580/)

- Attending to Entities for Better Text Understanding: [LINK](https://arxiv.org/abs/1911.04361)

- Integrating Physiological Time Series and Clinical Notes with Deep Learning for Improved ICU Mortality Prediction: [Link](https://arxiv.org/pdf/2003.11059.pdf)

- Predictive Uncertainty Estimation via Prior Networks: [Link](https://papers.nips.cc/paper/7936-predictive-uncertainty-estimation-via-prior-networks.pdf)

- Latent Retrieval for Weakly Supervised Open Domain Question Answering: [Link](https://arxiv.org/abs/1906.00300)

- BATCHENSEMBLE: An Alternative Approach to Efficient Ensemble and Lifelong Learning: [Link](https://openreview.net/pdf?id=Sklf1yrYDr)

- Learning to Deceive with Attention-Based Explanations [LINK](https://arxiv.org/abs/1909.07913)

- Predicting Performance for Natural Language Processing Tasks [LINK](https://arxiv.org/abs/2005.00870)

- ConCare: Personalized Clinical Feature Embedding via Capturing the Healthcare Context [LINK](https://arxiv.org/pdf/1911.12216.pdf)

- Relation of the Relations: A New Paradigm of the Relation Extraction Problem [LINK](https://arxiv.org/abs/2006.03719)

- Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting [LINK](https://arxiv.org/pdf/1709.01604.pdf)

- Word-level Textual Adversarial Attacking as Combinatorial Optimization [LINK](https://www.aclweb.org/anthology/2020.acl-main.540.pdf)

- A Reinforced Generation of Adversarial Examples for Neural Machine Translation [LINK](https://www.aclweb.org/anthology/2020.acl-main.319.pdf)

- A Simple and Effective Unified Encoder for Document-Level Machine Translation [LINK](https://www.aclweb.org/anthology/2020.acl-main.321.pdf)

- Shortcut Learning in Deep Neural Network [LINK](https://arxiv.org/pdf/2004.07780.pdf)

- Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks [LINK](https://www.aclweb.org/anthology/2020.acl-main.268.pdf)

- Sum Product Networks [LINK](https://arxiv.org/pdf/1202.3732.pdf)

- Learning Conceptual-Contextual Embeddings for Medical Text [LINK](https://arxiv.org/abs/1908.06203)

- Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering [LINK](https://arxiv.org/pdf/2007.01282.pdf)

-  Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts [LINK](https://www.aclweb.org/anthology/2020.acl-main.2.pdf)

- Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering [LINK](https://arxiv.org/abs/1909.05311)

- ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning [LINK](https://arxiv.org/abs/1811.00146)

- Why is my classifier discriminatory? [LINK](https://arxiv.org/pdf/1805.12002.pdf)

- Disagreement-Regularized Imitation Learning [LINK](https://openreview.net/pdf?id=rkgbYyHtwB) 

- BART [LINK](https://arxiv.org/abs/1910.13461)

- A hybrid method of recurrent neural network and graph neural network for next-period prescription prediction [LINK](https://link.springer.com/article/10.1007/s13042-020-01155-x)

- Graph Neural Network-Based Diagnosis Prediction [LINK](https://www.liebertpub.com/doi/10.1089/big.2020.0070)

- Reading Wikipedia to Answer Open-Domain Questions

- REALM: Retrieval-Augmented Language Model Pre-Training

- Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

- Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering

- The Lottery Ticket Hypothesis for Pre-trained BERT Networks [LINK](https://arxiv.org/abs/2007.12223)

- DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter [LINK](https://arxiv.org/pdf/1910.01108) 

- TinyBERT: Distilling BERT fro Natural Language Understanding [LINK](https://arxiv.org/abs/1909.10351)

- Not Enough Data? Deep Learning to the Rescue! [LINK](https://arxiv.org/abs/1911.03118)

- Unsupervised Domain Adaptation of a Pretrained Cross-lingual Language Model

- Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-training

- An Unsupervised Sentence Embedding Method by Mutual Information Maximization

- Generating Accurate Electronic Health Assessment from Medical Graph

- Med-BERT: pre-trained contextualized embeddings on large-scale structuredelectronic health records for disease prediction.

- Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer (NAACL-HLT 2018)

- Harnessing Pre-Trained Neural Networks with Rules for Formality Style Transfer (EMNLP-2019) 3.Parallel Data Augmentation for Formality Style Transfer

- Efficient Transformers: A Survey [LINK](https://arxiv.org/abs/2009.06732)

- Serial Recall Effects in Neural Language Modeling [LINK](https://www.aclweb.org/anthology/N19-1073.pdf)

- Federated Learning of Electronic Health Records Improves Mortality Prediction in Patients Hospitalized with COVID-19 [LINK](https://www.medrxiv.org/content/medrxiv/early/2020/08/14/2020.08.11.20172809.full.pdf)

- Learning Graphical Structure of Electronic Health Records with Transformer for Predictive Healthcare [LINK](https://arxiv.org/abs/1906.04716)

- Pointer Graph Networks [LINK](https://arxiv.org/abs/2006.06380)  

- Understanding Black-box Predictions via Influence Functions [LINK](https://arxiv.org/pdf/1703.04730.pdf)

- [Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge](https://arxiv.org/abs/2006.06609)

- EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets [LINK](https://arxiv.org/abs/2101.00063)

- [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/pdf/2008.02496.pdf)

## Some related papers from discussion

- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: [LINK](https://arxiv.org/abs/1810.04805)

- XLNet: Generalized Autoregressive Pretraining for Language Understanding: [LINK](https://arxiv.org/abs/1906.08237)

- Universal Transformers: [LINK](https://arxiv.org/abs/1807.03819)

## Arxiv Links to ACL2020 papers collated at a single place: [LINK](https://github.com/roomylee/ACL-2020-Papers)
