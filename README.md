# List of papers presented during Journal Club | BioNLP Lab, UMass-Amherst and Text Machine Lab, UMass Lowell.

- [Improved Pretraining for Domain-specific Contextual Embedding Models](https://arxiv.org/abs/2004.02288)

- [Gradient Episodic Memory for Continual Learning](https://papers.nips.cc/paper/7225-gradient-episodic-memory-for-continual-learning.pdf)

- [THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS](https://www.thetalkingmachines.com/sites/default/files/2019-05/2c35994ea2912e6517a87c50fc55faa58f0df150-compressed.pdf)

- [Span-based Joint Entity and Relation Extraction with Transformer Pre-training](https://arxiv.org/pdf/1909.07755.pdf)

- [CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning](https://arxiv.org/pdf/1911.10438.pdf)

- [Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction](https://arxiv.org/pdf/1911.09886.pdf)

- [Composition-based Multi-Relational Graph Convolutional Networks](https://arxiv.org/abs/1911.03082)

- [Linguistically-Informed Self-Attention for Semantic Role Labeling](https://arxiv.org/abs/1804.08199)

- [Attention Guided Graph Convolutional Networks for Relation Extraction](https://arxiv.org/abs/1906.07510)

- [When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion](https://www.aclweb.org/anthology/P19-1116/), [BLOG](https://lena-voita.github.io/posts/acl19_context.html)

- [Probing Neural Network Comprehension of Natural Language Arguments](https://arxiv.org/pdf/1907.07355.pdf)

- [SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability](https://arxiv.org/abs/1706.05806)

- [Attentive History Selection for Conversational Question Answering](https://arxiv.org/abs/1908.09456)

- [On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140#sec019)

- [Are Sixteen Heads Really Better than One?](https://arxiv.org/pdf/1905.10650.pdf)

- [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://www.aclweb.org/anthology/P19-1580/)

- [Attending to Entities for Better Text Understanding](https://arxiv.org/abs/1911.04361)

- [Integrating Physiological Time Series and Clinical Notes with Deep Learning for Improved ICU Mortality Prediction](https://arxiv.org/pdf/2003.11059.pdf)

- [Predictive Uncertainty Estimation via Prior Networks](https://papers.nips.cc/paper/7936-predictive-uncertainty-estimation-via-prior-networks.pdf)

- [Latent Retrieval for Weakly Supervised Open Domain Question Answering](https://arxiv.org/abs/1906.00300)

- [BATCHENSEMBLE: An Alternative Approach to Efficient Ensemble and Lifelong Learning](https://openreview.net/pdf?id=Sklf1yrYDr)

- [Learning to Deceive with Attention-Based Explanations](https://arxiv.org/abs/1909.07913)

- [Predicting Performance for Natural Language Processing Tasks](https://arxiv.org/abs/2005.00870)

- [ConCare: Personalized Clinical Feature Embedding via Capturing the Healthcare Context](https://arxiv.org/pdf/1911.12216.pdf)

- [Relation of the Relations: A New Paradigm of the Relation Extraction Problem](https://arxiv.org/abs/2006.03719)

- [Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting](https://arxiv.org/pdf/1709.01604.pdf)

- [Word-level Textual Adversarial Attacking as Combinatorial Optimization](https://www.aclweb.org/anthology/2020.acl-main.540.pdf)

- [A Reinforced Generation of Adversarial Examples for Neural Machine Translation](https://www.aclweb.org/anthology/2020.acl-main.319.pdf)

- [A Simple and Effective Unified Encoder for Document-Level Machine Translation](https://www.aclweb.org/anthology/2020.acl-main.321.pdf)

- [Shortcut Learning in Deep Neural Network](https://arxiv.org/pdf/2004.07780.pdf)

- [Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks](https://www.aclweb.org/anthology/2020.acl-main.268.pdf)

- [Sum Product Networks](https://arxiv.org/pdf/1202.3732.pdf)

- [Learning Conceptual-Contextual Embeddings for Medical Text](https://arxiv.org/abs/1908.06203)

- [Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering](https://arxiv.org/pdf/2007.01282.pdf)

-  [Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts](https://www.aclweb.org/anthology/2020.acl-main.2.pdf)

- [Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering](https://arxiv.org/abs/1909.05311)

- [ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning](https://arxiv.org/abs/1811.00146)

- [Why is my classifier discriminatory?](https://arxiv.org/pdf/1805.12002.pdf)

- [Disagreement-Regularized Imitation Learning](https://openreview.net/pdf?id=rkgbYyHtwB) 

- [BART](https://arxiv.org/abs/1910.13461)

- [A hybrid method of recurrent neural network and graph neural network for next-period prescription prediction](https://link.springer.com/article/10.1007/s13042-020-01155-x)

- [Graph Neural Network-Based Diagnosis Prediction](https://www.liebertpub.com/doi/10.1089/big.2020.0070)

- Reading Wikipedia to Answer Open-Domain Questions

- REALM: Retrieval-Augmented Language Model Pre-Training

- Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

- Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering

- [The Lottery Ticket Hypothesis for Pre-trained BERT Networks](https://arxiv.org/abs/2007.12223)

- [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108) 

- [TinyBERT: Distilling BERT fro Natural Language Understanding](https://arxiv.org/abs/1909.10351)

- [Not Enough Data? Deep Learning to the Rescue!](https://arxiv.org/abs/1911.03118)

- Unsupervised Domain Adaptation of a Pretrained Cross-lingual Language Model

- Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-training

- An Unsupervised Sentence Embedding Method by Mutual Information Maximization

- Generating Accurate Electronic Health Assessment from Medical Graph

- Med-BERT: pre-trained contextualized embeddings on large-scale structuredelectronic health records for disease prediction.

- Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer (NAACL-HLT 2018)

- Harnessing Pre-Trained Neural Networks with Rules for Formality Style Transfer (EMNLP-2019) 3.Parallel Data Augmentation for Formality Style Transfer

- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)

- [Serial Recall Effects in Neural Language Modeling](https://www.aclweb.org/anthology/N19-1073.pdf)

- [Federated Learning of Electronic Health Records Improves Mortality Prediction in Patients Hospitalized with COVID-19](https://www.medrxiv.org/content/medrxiv/early/2020/08/14/2020.08.11.20172809.full.pdf)

- [Learning Graphical Structure of Electronic Health Records with Transformer for Predictive Healthcare](https://arxiv.org/abs/1906.04716)

- [Pointer Graph Networks](https://arxiv.org/abs/2006.06380)  

- [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/pdf/1703.04730.pdf)

- [Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge](https://arxiv.org/abs/2006.06609)

- [EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets](https://arxiv.org/abs/2101.00063). Presentation:[PPT](https://docs.google.com/presentation/d/10lJS5rCLQvNmesLOfkQeeUq452MzTALLOQqjygCw_y4/edit?usp=sharing)
-- Relevant reading: https://arxiv.org/abs/2007.12223, https://arxiv.org/pdf/2005.00561

- [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/pdf/2008.02496.pdf)

## Some related papers from discussion

- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: [LINK](https://arxiv.org/abs/1810.04805)

- XLNet: Generalized Autoregressive Pretraining for Language Understanding: [LINK](https://arxiv.org/abs/1906.08237)

- Universal Transformers: [LINK](https://arxiv.org/abs/1807.03819)

## Arxiv Links to ACL2020 papers collated at a single place: [LINK](https://github.com/roomylee/ACL-2020-Papers)
