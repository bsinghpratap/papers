# List of papers presented during Journal Club | BioNLP Lab, UMass-Amherst.

- Improved Pretraining for Domain-specific Contextual Embedding Models: [LINK](https://arxiv.org/abs/2004.02288)

- Gradient Episodic Memory for Continual Learning: [LINK](https://papers.nips.cc/paper/7225-gradient-episodic-memory-for-continual-learning.pdf)

- THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS: [LINK](https://www.thetalkingmachines.com/sites/default/files/2019-05/2c35994ea2912e6517a87c50fc55faa58f0df150-compressed.pdf)

- Span-based Joint Entity and Relation Extraction with Transformer Pre-training: [LINK](https://arxiv.org/pdf/1909.07755.pdf)

- CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning: [LINK](https://arxiv.org/pdf/1911.10438.pdf)

- Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction: [LINK](https://arxiv.org/pdf/1911.09886.pdf)

- Composition-based Multi-Relational Graph Convolutional Networks: [LINK](https://arxiv.org/abs/1911.03082)

- Linguistically-Informed Self-Attention for Semantic Role Labeling: [LINK](https://arxiv.org/abs/1804.08199)

- Attention Guided Graph Convolutional Networks for Relation Extraction: [LINK](https://arxiv.org/abs/1906.07510)

- When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion: [LINK](https://www.aclweb.org/anthology/P19-1116/), [BLOG](https://lena-voita.github.io/posts/acl19_context.html)

- Probing Neural Network Comprehension of Natural Language Arguments: [LINK](https://arxiv.org/pdf/1907.07355.pdf)

- SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability: [LINK](https://arxiv.org/abs/1706.05806)

- Attentive History Selection for Conversational Question Answering: [LINK](https://arxiv.org/abs/1908.09456)


## Some related papers from discussion

- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: [LINK](https://arxiv.org/abs/1810.04805)

- XLNet: Generalized Autoregressive Pretraining for Language Understanding: [LINK](https://arxiv.org/abs/1906.08237)

- Universal Transformers: [LINK](https://arxiv.org/abs/1807.03819)

